---
title: "Strokes"
author: "Felipe Maggi"
date: "16/7/2021"
output:
  pdf_document: default
  html_document: default
---

```{r packages, message=FALSE, include=FALSE}

# _______________________########
# PACKAGES AND LIBRARIES ########
# _______________________########

#__reader #####
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
library(readr)

#__tidyverse #####
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
library(tidyverse)

#__caret #####
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
library(caret)

#__igraph ####
if(!require(igraph)) install.packages("igraph", repos = "http://cran.us.r-project.org")
library(igraph)

#__rattle ####
if(!require(rattle)) install.packages("rattle", repos = "http://cran.us.r-project.org")
library(rattle)

#__randomForest ####
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
library(randomForest)

#__fastAdaboost ####
if(!require(fastAdaboost)) install.packages("fastAdaboost", repos = "http://cran.us.r-project.org")
library(fastAdaboost)

#__ROSE ####
if(!require(ROSE)) install.packages("ROSE", repos = "http://cran.us.r-project.org")
library(ROSE)

#__mda ####
if(!require(mda)) install.packages("mda", repos = "http://cran.us.r-project.org")
library(mda)

#__klaR ####
if(!require(klaR)) install.packages("klaR", repos = "http://cran.us.r-project.org")
library(klaR)

#__nnet ####
if(!require(nnet)) install.packages("nnet", repos = "http://cran.us.r-project.org")
library(nnet)

#__kernlab ####
if(!require(kernlab)) install.packages("kernlab", repos = "http://cran.us.r-project.org")
library(kernlab)

#__e1071 ####
if(!require(e1071)) install.packages("e1071", repos = "http://cran.us.r-project.org")
library(e1071)

#__viridis ####
if(!require(viridis)) install.packages("viridis", repos = "http://cran.us.r-project.org")
library(viridis)

#__patchwork####
if(!require(patchwork)) install.packages("patchwork", repos = "http://cran.us.r-project.org")
library(patchwork)

#__hrbrthemes####
if(!require(hrbrthemes)) install.packages("hrbrthemes", repos = "http://cran.us.r-project.org")
library(hrbrthemes)

#__ggraph####
if(!require(ggraph)) install.packages("ggraph", repos = "http://cran.us.r-project.org")
library(ggraph)

#__readxl####
if(!require(readxl)) install.packages("readxl", repos = "http://cran.us.r-project.org")
library(readxl)

#__knitr####
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
library(knitr)

#__tidyr####
if(!require(tidyr)) install.packages("tidyr", repos = "http://cran.us.r-project.org")
library(tidyr)

#__dplyr####
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
library(dplyr)

#__plotly####
if(!require(plotly)) install.packages("plotly", repos = "http://cran.us.r-project.org")
library(plotly)

#__magrittr####
if(!require(magrittr)) install.packages("plotly", repos = "http://cran.us.r-project.org")
library(magrittr)

#__lubridate####
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
library(lubridate)

#__rvest####
if(!require(rvest)) install.packages("rvest", repos = "http://cran.us.r-project.org")
library(rvest)

#__rpart####
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org")
library(rpart)

#__rpart.plot####
if(!require(rpart.plot)) install.packages("rpart.plot", repos = "http://cran.us.r-project.org")
library(rpart.plot)

#__pROC####
if(!require(pROC)) install.packages("pROC", repos = "http://cran.us.r-project.org")
library(pROC)

#__nnet####
if(!require(nnet)) install.packages("nnet", repos = "http://cran.us.r-project.org")
library(nnet)

#__earth####
if(!require(earth)) install.packages("earth", repos = "http://cran.us.r-project.org")
library(earth)

#__LiblineaR####
if(!require(LiblineaR)) install.packages("LiblineaR", repos = "http://cran.us.r-project.org")
library(LiblineaR)

#__MLeval####
if(!require(MLeval)) install.packages("MLeval", repos = "http://cran.us.r-project.org")

#__Naives Bayes####
if(!require(naivebayes)) install.packages("naivebayes", repos = "http://cran.us.r-project.org")
library(naivebayes) #cambio

#__Recipes####
if(!require(recipes)) install.packages("recipes", repos = "http://cran.us.r-project.org")
library(recipes)

#__Rattle####
if(!require(rattle)) install.packages("rattle", repos = "http://cran.us.r-project.org")
library(rattle)

```

***
### Abstract
***

# 1. Introduction

## 1.1 Dataset description
The objective of this project is to develop a model capable of predicting whether a patient has a tendency to suffer a stroke, based on the following variables:

1. Gender (binary)
2. Age (numerical)
3. Hypertension (binary)
4. Heart disease (binary)
5. Ever married (binary)
6. Type of residence (binary)
7. Average glucose level (numerical)
8. Body mass index (numerical)
9. Smoking status (categorical)

The dataset used (Stroke Prediction Data Data Set) contains 5,110 observations. It is available in Kaggle [^1], and was uploaded to the platform by Federico Soriano (fedesorian) [^2]. 

Originally, the variables and their respective classes are as follows:

```{r data_download, echo=FALSE, message=FALSE}

# _______________________########
# DATA DOWNLOAD ########
# _______________________########

# stroke_data from github #####

stroke_data_orig <- 
  read.csv("https://raw.github.com/felmaggilab/edx_data_science_capstone_strokes/master/data/healthcare-dataset-stroke-data.csv")

stroke_data <- 
  read_csv("https://raw.github.com/felmaggilab/edx_data_science_capstone_strokes/master/data/healthcare-dataset-stroke-data.csv")

str(stroke_data_orig)

```

In the following sections we will explain the changes that were made to the original dataset, in order to apply the selected models.

To train the models, the final dataset was divided into a training set and a test set, in a ratio of 80-20. We decided to reserve at least 20% of the observations for the test set because the dataset is not excessively large and, as we will see, it is highly unbalanced. With a lower percentage of observations, we ran the risk of not having enough stroke cases in the test set, or that certain variables would see their proportions significantly affected.

Unfortunately, we think we don't have enough observations to reserve a part of the dataset and use it as a validation set. Because of this, we have only worked with the training and test sets, and we have relied on cross-validation.

## 1.2 Goals and key steps
As we have already mentioned, the objective of this project is to train a model capable of predicting whether a patient has a high probability of suffering a stroke, taking into account the mentioned predictors.

The key steps in the development of this project were:

1. Treat the original dataset, cleaning data and eliminating superfluous variables like "id".
2. Carry out an exploratory analysis of the data
3. Determine the importance of the selected variables
4. Center and scale the numeric variables
5. Test provisional results, training and visualizing various models with the original unbalanced dataset.
6. Balance the training set using various techniques.
7. Train the classification models with the different training sets obtained with the modes used in the previous point.
8. Create ensembles of the best performing models for each balancing technique, and select a stroke prediction criterion based on the balanced accuracy metric.

# 2. Methods and Analysis

## 2.1 Data wrangling
During the development of this project, the data cleansing was carried out in several phases. In the first place, the variable "id" was eliminated, for not providing relevant information. At the same time, observations containing N/A (a problem affecting the BMI variable) were filtered, and categorical variables were factored (0 and 1 were converted to "No" or "Yes", respectively). 

After eliminating the N/As the BMI variable was transformed into numeric (in the original data set it had been considered as a character by coercion).

Finally, the stroke variable was factored ("no_stroke", "stroke"), and the levels were reordered so that "stroke" became the positive class. 

The resulting dataset now contains 4,909 observations. The 201 deleted observations correspond to those in which BMI was reported as N/A:

```{r data_wrangling, echo=FALSE, message=FALSE}

# _______________________########
# DATA WRANGLING ########
# _______________________########

# Categorical and binary data as.factors #####

stroke_data <- stroke_data %>% 
  filter(!bmi == "N/A")  %>% # filtering bmi = N/A
  mutate(gender = as.factor(gender),
         hypertension = as.factor(ifelse(hypertension == 0, "No", "Yes")),
         heart_disease = as.factor(ifelse(heart_disease == 0, "No", "Yes")),
         ever_married = as.factor(ever_married),
         work_type = as.factor(work_type),
         Residence_type = as.factor(Residence_type),
         bmi = as.numeric(bmi),
         smoking_status = as.factor(smoking_status),
         stroke = as.factor(ifelse(stroke == 1, "stroke", "no_stroke"))) %>% 
  select(!id) # Removing "id" variable

# Relevel "stroke" "no_stroke" factors: positive class: "stroke" #### 
# It makes "stroke" the positive class, in order to facilitate interpretations

stroke_data$stroke <- relevel(stroke_data$stroke, ref = "stroke")

str(stroke_data)

```

Then, during data exploration, it was discovered that there was an observation whose variable "gender" was reported as "Other". This observation was also removed. The hypothesis here is that this observation would add noise if gender was an important variable. We do not want this to be misunderstood. Filtering this class is for training purposes only. A single observation can generate significant changes in the output of the models (something we check). The author unconditionally respects any sexual preference.

Before starting to train models, the numerical variables were scaled and centralized.

The last modifications made to the dataset was to balance the "stroke" and "no_stroke" classes, with different methods. We will see the details later.

## 2.2 Data exploration

For certain parts of this exploratory analysis we have followed the recommendations of Joaquín Amat Rodrigo, exposed on the site *Machine Learning with R and caret*[^3]. Specifically, with regard to: 

1. The statistical data of the numerical variables
2. The Density and box plots
3. The distribution of the qualitative variables
4. The correlation between numerical variables
5. The contrast of proportions for categorical variables
6. The random forest method
7. The Near Zero Variance Analysis

### 2.2.1 Viariable distributions

 The first thing that stands out during the exploration of the data, is the clear imbalance between the classes "stroke" and "no_stroke". To maintain the clarity of the exposition, and the correspondence with the code, we show the data before filtering the observation with gender "Other".

```{r stroke_porportions, echo=FALSE}

n <- nrow(stroke_data)

# Strokes proportion #####
table(stroke_data$stroke) %>% 
  kable()

prop.table(table(stroke_data$stroke)) %>% 
  kable()

```

Only 4% of the observations are labeled "stroke", so the prevalence of the "no_stroke" class is staggering (close to 96%).

This represents the first hurdle to overcome. With such a prevalence of the negative class, training any model keeping the proportions of the original dataset will not give good results.

Obviously, the "accuracy" metric loses all sense. A model that predicts "no_stroke" in all cases will have an accuracy close to 96%. The important thing here is the sensitivity of the model (remember that "stroke" is the positive class). But, at the same time, we want to obtain a model that has a correct specificity. Predicting "stroke" in all cases would raise the sensitivity to 100%, but would leave us with another useless model unable to correctly classify any negative cases.

Anyway, we will analyze the distribution of the variables in this dataset, and then we will apply various methods to balance the training set. This step is necessary to check if the changes made to the training set do not alter the statistical data of the predictor variables.

#### 2.2.1.1 Age

The age statistics, depending on the response variables, are the following:

```{r age_statistics, echo=FALSE, message=FALSE}

# __Statistical Age Data  ####
stroke_data %>% 
  group_by(stroke) %>% 
  summarise(avg_age = round(mean(age),1),
            median_age = round(median(age)),
            min_age = min(age),
            max_age = max(age)) %>% 
  kable()

```

It is not surprising that age, as we shall see, is the most important variable when classifying subjects according to "stroke" and "no_stroke". However, the overlap in the age range already indicates another obstacle that affect the models obtained from the selected dataset. This overlap is clearly observed in the following density graph:

```{r age_density_plot, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Density Plot: Avg Level per Class ####
stroke_data %>% ggplot(aes(age, fill = stroke)) +
  geom_density(alpha = 0.2, bw = 1) +
  labs(title = "Age density plot") +
  theme(plot.title = element_text(size = 12, face = "bold")) +
  theme(plot.margin = unit(c(1,0,1,0), "cm"))

```

It is clear that age is an important factor, but as a variable it is far from dividing the observations clearly between one class and another.

The same is observed by means of box plots. As we are working with the original dataset, the prevalence of the "no_stroke" class represented by the point density is also clearly visible.

```{r age_boox_plot, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Box Plot: Avg Level per Class ####
stroke_data %>% ggplot(aes(stroke, age, color = stroke)) +
  geom_boxplot() +
  geom_jitter(alpha = 0.3, width = 0.15) +
  labs(title = "Age box plot") +
  theme(plot.title = element_text(size = 12, face = "bold")) +
  theme(plot.margin = unit(c(1,0,1,0), "cm"))

```

Regarding the number of observations according to age, we see that for very short ages (up to approximately 2 years old) there are few data. From two years old onwards we have, with few exceptions, over 35 observations. The models were trained without grouping ages, and this tactic could be one of the changes to be tested to improve the results obtained.

```{r dist_obs_age, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Distribution of observations by age #####
stroke_data %>% 
  group_by(age) %>%
  summarise(age = age, total = n(), strokes = sum(stroke == "stroke"),
            stroke_ratio = mean(stroke == "stroke")) %>% 
  #filter(!stroke_ratio == 0) %>% 
  unique() %>%
  ggplot(aes(x=age, y=total)) +
  geom_segment(aes(x=age, xend=age, y=0, yend=total), color="skyblue") +
  geom_point( color="blue", size=2, alpha=0.6) +
  theme_light() +
  #coord_flip() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  labs(title = "Number of Observations by Age") +
  theme(plot.title = element_text(size = 10, face = "bold")) +
  theme(plot.margin = unit(c(1,0,1,0), "cm"))

```

The percentage of strokes by age shows some correlation between this variable and the tendency to suffer from this disease:

```{r percent_strokes_by_age, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Percent of strokes by age ####
stroke_data %>% 
  group_by(age) %>%
  summarise(age = age, total = n(), strokes = sum(stroke == "stroke"),
            stroke_percent = mean(stroke == "stroke")) %>% 
  filter(!stroke_percent == 0) %>% 
  unique() %>%
  ggplot(aes(x=age, y=stroke_percent)) +
  geom_segment(aes(x=age, xend=age, y=0, yend=stroke_percent), color="skyblue") +
  geom_point(color="blue", size=4, alpha=0.6) +
  theme_light() +
  # coord_flip() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  labs(title = "Percent of Strokes by Age") +
  theme(plot.title = element_text(size = 10, face = "bold")) +
  theme(plot.margin = unit(c(1,0,1,0), "cm"))
```

As we have said, the models were trained with the ungrouped age data. If we group the ages to the nearest ten, the information acquires much more meaning and representativeness.

```{r summary_table_by_age, echo=FALSE, message=FALSE}

# __Summary table by age (rounded nearest 10) #####
# age, total of observations, number of strokes, percent of strokes
stroke_data %>% 
  group_by(round_age = round(age, -1)) %>%
  summarise(total = n(), percent = round(total/n, 3), strokes = sum(stroke == "stroke"),
            stroke_percent = round(mean(stroke =="stroke"), 3)) %>% 
  unique() %>%
  knitr::kable()
```

The correlation between age and incidence of strokes now looks much better:

```{r percent_strokes_by_rounded_age, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Percent of strokes by rounded age  ####  
stroke_data %>% 
  group_by(age = round(age, -1)) %>%
  summarise(total = n(), percent = round(total/n, 3), strokes = sum(stroke == "stroke"),
            stroke_percent = round(mean(stroke == "stroke"), 3)) %>% 
  unique() %>%
  ggplot(aes(x=age, y=stroke_percent)) +
  geom_segment(aes(x=age, xend=age, y=0, yend=stroke_percent), color="skyblue") +
  geom_point(color="blue", size=4, alpha=0.6) +
  theme_light() +
  # coord_flip() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  labs(title = "Percent of Strokes by Age") +
  theme(plot.title = element_text(size = 10, face = "bold")) +
  theme(plot.margin = unit(c(1,0,1,0), "cm"))

```

Why, if knowing this, were the models trained the ungrouped age? First, to determine to what extent the models are able to deal with the original data and, second, to have improvement options.

#### 2.2.1.2 Average Glucose Level

Some of the problems detected in the case of age are even more serious in the case of the average glucose level. There are some differences in mean and median, but the ranges between minimum and maximum clearly overlap:

```{r avg_glucose_statistics, echo=FALSE, message=FALSE}

# __Statistical Avg Glucose Level Data  ####
stroke_data %>% 
  group_by(stroke) %>% 
  summarise(avg_glucose = round(mean(avg_glucose_level),1),
            median_glucose = round(median(avg_glucose_level)),
            min_glucose = min(avg_glucose_level),
            max_glucose = max(avg_glucose_level)) %>% 
  kable()
```

```{r glucose_density_plot, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Density Plot: Avg Level per Class ####
stroke_data %>% ggplot(aes(avg_glucose_level, fill = stroke)) +
  geom_density(alpha = 0.2, bw = 5) +
  labs(title = "Avg Glucose Level density plot") +
  theme(plot.title = element_text(size = 10, face = "bold")) +
  theme(plot.margin = unit(c(1,0,1,0), "cm"))

```

In addition, we are facing a clear bimodal case, which affects both classes.

The box plot shows that the medians do not differ that much, and that the interquartile ranges overlap, despite having different sizes:

```{r glucose_box_plot, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Box Plot: Avg Level per Class ####
stroke_data %>% ggplot(aes(stroke, avg_glucose_level, color = stroke)) +
  geom_boxplot() +
  geom_jitter(alpha = 0.3, width = 0.15) +
  labs(title = "Avg Glucose Level box plot") +
  theme(plot.title = element_text(size = 10, face = "bold")) +
  theme(plot.margin = unit(c(1,0,1,0), "cm"))
```

It is also important to highlight that with so few observations belonging to the "stroke" class, their variability is much greater, and it is perfectly possible that the data are not representative of the population. The latter is something that affects all variables, and balancing techniques do not solve. It is true that they increase the number of observations of the "stroke" class, but they do so by randomly repeating (or according to best estimate criteria) the available observations. Representativeness, or its lack of it, we understand that it is maintained.

In this case we have rounded to the nearest ten directly, although the models have been trained with the ungrouped data:

```{r summary_table_by_avg_glucose_level, echo=FALSE, message=FALSE}

# __Summary table by avg_glucose_level (round to nearest ten) #####
# avg_glucose_level, total of observations, number of strokes, percent of strokes
stroke_data %>% 
  group_by(round_avg_glucose_level = round(avg_glucose_level, -1)) %>%
  summarise(total = n(), percent = round(total/n, 3), strokes = sum(stroke == "stroke"),
            stroke_percent = round(mean(stroke == "stroke"), 3)) %>% 
  unique() %>%
  knitr::kable()

```

The number of observations according to the rounded glucose average are concentrated between 60 and 110, from there they are much scarcer, although they show a slight increase between 190 and 230:

```{r dist_obs_rounded avg_glucose, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Distribution of observations by rounded avg_glucose_level  #####
stroke_data %>% 
  group_by(avg_glucose_level = round(avg_glucose_level, -1)) %>%
  summarise(total = n(), percent = round(total/n, 3), strokes = sum(stroke == "stroke"),
            stroke_percent = round(mean(stroke == "stroke"), 3)) %>% 
  unique() %>%
  ggplot(aes(x=avg_glucose_level, y=total)) +
  geom_segment(aes(x=avg_glucose_level, xend=avg_glucose_level, y=0, yend=total), color="skyblue") +
  geom_point( color="blue", size=4, alpha=0.6) +
  theme_light() +
  # coord_flip() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  labs(title = "Number of Observations by avg_glucose_level (Rounded)") +
  theme(plot.title = element_text(size = 10, face = "bold")) +
  theme(plot.margin = unit(c(1,0,1,0), "cm"))
```

Regarding the incidence of strokes according to rounded average glucose level, we observe a positive correlation (not as clear as in the case of age)

```{r percent_strokes_by_rounded_avg_glucose, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Percent of strokes by rounded avg_glucose_level ####  
stroke_data %>% 
  group_by(avg_glucose_level = round(avg_glucose_level, -1)) %>%
  summarise(total = n(), percent = round(total/n, 3), strokes = sum(stroke == "stroke"),
            stroke_percent = round(mean(stroke == "stroke"), 3)) %>% 
  unique() %>%
  ggplot(aes(x=avg_glucose_level, y=stroke_percent)) +
  geom_segment(aes(x=avg_glucose_level, xend=avg_glucose_level, y=0, yend=stroke_percent), color="skyblue") +
  geom_point(color="blue", size=4, alpha=0.6) +
  theme_light() +
  # coord_flip() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  labs(title = "Percent of Strokes by avg_glucose_level (Rounded)") +
  theme(plot.title = element_text(size = 10, face = "bold")) +
  theme(plot.margin = unit(c(1,0,1,0), "cm"))

```

#### 2.2.1.3 Body Mass Index

The last of the numeric variables is the most complex, in the sense that it doesn't seem to separate the classes at all.

The mean and median are very similar, and the minimum-maximum range for the "no_stroke" class completely covers the range for the "stroke" class:

```{r bmi_statistics, echo=FALSE, message=FALSE}

# __Statistical BMI Data  ####
stroke_data %>% 
  group_by(stroke) %>% 
  summarise(avg_bmi = round(mean(bmi),1),
            median_bmi = round(median(bmi)),
            min_bmi = min(bmi),
            max_bmi = max(bmi)) %>% 
  kable()

```

The density and box plots make this much clearer:

```{r bmi_density_plot, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Density Plot: BMI per Class ####
stroke_data %>% ggplot(aes(bmi, fill = stroke)) +
  geom_density(alpha = 0.2, bw = 1) +
  labs(title = "BMI density plot") +
  theme(plot.title = element_text(size = 12, face = "bold")) +
  theme(plot.margin = unit(c(1,0,1,0), "cm"))

```

```{r bmi_box_plot, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Box Plot: BMI per Class ####
stroke_data %>% ggplot(aes(stroke, bmi, color = stroke)) +
  geom_boxplot() +
  geom_jitter(alpha = 0.3, width = 0.15)  +
  labs(title = "BMI box plot") +
  theme(plot.title = element_text(size = 12, face = "bold")) +
  theme(plot.margin = unit(c(1,0,1,0), "cm"))

```

This variable, despite everything, seems to be important according to the random forest variable selection method (Amat, 2018), which we will see later.

In any case, the random forest model did not yield good results with any of the balancing techniques. A possible course of action when it comes to improving the models is to test the training without this variable.

The BMI table rounded to the nearest ten shows very few observations starting at 70, and that the incidence is concentrated between 20 and 40.

```{r summary_table_by_rounded_bmi, echo=FALSE, message=FALSE}

# __Summary table by bmi (round to nearest ten) #####
# bmi, total of observations, number of strokes, percent of strokes
stroke_data %>% 
  mutate(bmi = as.numeric(bmi)) %>% 
  group_by(round_bmi = round(bmi,-1)) %>%
  summarise(total = n(), percent = round(total/n, 3), strokes = sum(stroke == "stroke"),
            stroke_percent = round(mean(stroke == "stroke"), 3)) %>% 
  unique() %>%
  knitr::kable()

```

In any case, we are talking about percentages ranging from 2% to 5%, which reinforces the hypothesis that this variable, in this specific dataset, should not have a great weight.

```{r percent_strokes_by_rounded_bmi, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Percent of strokes by rounded bmi ####  
stroke_data %>% 
  mutate(bmi = as.numeric(bmi)) %>% 
  group_by(bmi = round(bmi, -1)) %>%
  summarise(total = n(), percent = round(total/n, 3), strokes = sum(stroke == "stroke"),
            stroke_percent = round(mean(stroke == "stroke"), 3)) %>% 
  unique() %>%
  ggplot(aes(x=bmi, y=stroke_percent)) +
  geom_segment(aes(x=bmi, xend=bmi, y=0, yend=stroke_percent), color="skyblue") +
  geom_point(color="blue", size=4, alpha=0.6) +
  theme_light() +
  # coord_flip() +
  theme(
    panel.grid.major.y = element_blank(),
    panel.border = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  labs(title = "Percent of bmi (Rounded)") +
  theme(plot.title = element_text(size = 10, face = "bold")) +
  theme(plot.margin = unit(c(1,0,1,0), "cm"))

```

#### 2.2.1.4 Gender

Based on the available data, the incidence of strokes among men is slightly higher than among women. The difference is not great, and as we will see later, the variable has some importance but it is not among the main ones:

```{r summary_table_by_gender, echo=FALSE, message=FALSE}

# __Summary table by gender #####
# gender, total of observations, number of strokes, percent of strokes
stroke_data %>% 
  group_by(gender) %>%
  summarise(total = n(), percent = round(total/n, 3), 
            strokes = sum(stroke == "stroke"), 
            stroke_percent = round(mean(stroke == "stroke"), 3)) %>% 
  unique() %>%
  knitr::kable()

stroke_data <- stroke_data %>% filter(!gender == "Other") # Filtering gender "Other" for training purposes

```

The bar graphs clearly show the distribution of the variables, but due to the high prevalence of the "no_stroke" class, it is difficult to see that the incidence is actually somewhat higher among men, as shown in the table.

```{r gender_distribution, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Gender distribution #####
stroke_data %>% 
  ggplot(aes(x = gender, y = ..count.., fill = stroke)) +
  geom_bar() +
  scale_fill_manual(values = c("gray50", "orangered2")) +
  labs(title = "Gender distribution") +
  theme_bw() +
  theme(legend.position = "bottom")
```

#### 2.2.1.5 Hypertension

Due to the clear difference in the incidence of strokes depending on whether or not one suffers from hypertension, it is logical to think that this variable will have an important weight:

```{r summary_table_by_hypertencion, echo=FALSE, message=FALSE}

# __Summary table by hypertension #####
# hypertension, total of observations, number of strokes, percent of strokes
stroke_data %>% 
  group_by(hypertension) %>%
  summarise( total = n(), percent = round(total/n, 3), strokes = sum(stroke == "stroke"),
             stroke_percent = round(mean(stroke == "stroke"), 3)) %>% 
  unique() %>%
  knitr::kable()
```

Among those who do not suffer from hypertension, the incidence is 3%. Among those who do suffer it, it reaches 13%.

Again, it is difficult to see this reality in bar charts:

```{r hypertension_distribution, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Hypertension distribution #####
stroke_data %>% 
  ggplot(aes(x = hypertension, y = ..count.., fill = stroke)) +
  geom_bar() +
  scale_fill_manual(values = c("gray50", "orangered2")) +
  labs(title = "Hypertension distribution") +
  theme_bw() +
  theme(legend.position = "bottom")

```

#### 2.2.1.6 Heart Desease

Among those with heart disease, the incidence of strokes is around 17%, while among those who do not, the percentage of cases does not reach 4%:

```{r summary_table_by_heart_desease, echo=FALSE, message=FALSE}

# __Summary table by heart_disease #####
# heart_disease, total of observations, number of strokes, percent of strokes
stroke_data %>% 
  group_by(heart_disease) %>%
  summarise(total = n(), percent = round(total/n, 3), strokes = sum(stroke == "stroke"),
            stroke_percent = round(mean(stroke =="stroke"), 3)) %>% 
  unique() %>%
  knitr::kable()

```

```{r heart_desease_distribution, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Heart_disease distribution #####
stroke_data %>% 
  ggplot(aes(x = heart_disease, y = ..count.., fill = stroke)) +
  geom_bar() +
  scale_fill_manual(values = c("gray50", "orangered2")) +
  labs(title = "Heart disease distribution") +
  theme_bw() +
  theme(legend.position = "bottom")
```

Due to the clear difference in the probability of suffering a stroke depending on whether or not one has heart disease, it is reasonable to think that the variable will have a certain weight for the classification models.

When we get to the point of assessing the importance of variables, we will find certain surprises. Some variables that with this analysis seemed firm candidates to have a clear weight, in reality they do not, at least for those models in which the selection of variables is evident.

#### 2.2.1.7 Ever Married

There is also some difference in the probability of suffering a stroke whether you have been married or not. This difference is only partly surprising, if one takes into account that being married or not is directly related to age. Young people are obviously less likely to have been married than older people, and we have already seen that there is a clear relationship between age and strokes.

```{r summary_table_by_ever_married, echo=FALSE, message=FALSE}

# __Summary table by ever_married #####
# ever_married, total of observations, number of strokes, percent of strokes
stroke_data %>% 
  group_by(ever_married) %>%
  summarise(total = n(), percent = round(total/n, 3), strokes = sum(stroke == "stroke"),
            stroke_percent = round(mean(stroke == "stroke"), 3)) %>% 
  unique() %>%
  knitr::kable()

```

Among those who have been married, the percentage of strokes is around 6%. Among those who have always been single, the percentage barely exceeds 1%.

```{r ever_married_distribution, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# Ever_married distribution #####
stroke_data %>% 
  ggplot(aes(x = ever_married, y = ..count.., fill = stroke)) +
  geom_bar() +
  scale_fill_manual(values = c("gray50", "orangered2")) +
  labs(title = "Ever married distribution") +
  theme_bw() +
  theme(legend.position = "bottom")
```

#### 2.2.1.8 Work Type

Those who are self-employed appear to be at slightly higher risk of strokes than other types of work.

```{r summary_table_by_work_type, echo=FALSE, message=FALSE}

# __Summary table by work_type #####
# work_type, total of observations, number of strokes, percent of strokes
stroke_data %>% 
  group_by(work_type) %>%
  summarise(total = n(), percent = round(total/n, 3), strokes = sum(stroke == "stroke"),
            stroke_percent = round(mean(stroke == "stroke"), 3)) %>% 
  unique() %>%
  knitr::kable()

```

The cases of never worked are very few, and there are no incidences of strokes in this group. Children also do not present cases. This would explain why certain models, when they take this variable into account, use the group formed by Govt_job, Private and Self-employed as a single decision element.

```{r work_type_distribution, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Work_type distribution #####
stroke_data %>% 
  ggplot(aes(x = work_type, y = ..count.., fill = stroke)) +
  geom_bar() +
  scale_fill_manual(values = c("gray50", "orangered2")) +
  labs(title = "Work type distribution") +
  theme_bw() +
  theme(legend.position = "bottom")
```

#### 2.2.1.9 Residense Type

The type of residence does not seem to influence the risk of suffering a stroke. Both groups are very balanced, and the percentage of strokes in the two cases is very similar to the baseline incidence.

```{r summary_table_by_residence_type, echo=FALSE, message=FALSE}

# Summary table by Residence_type #####
# age, total of observations, number of strokes, percent of strokes
stroke_data %>% 
  group_by(Residence_type) %>%
  summarise(total = n(), percent = round(total/n, 3), strokes = sum(stroke == "stroke"),
            stroke_percent = round(mean(stroke == "stroke"), 3)) %>% 
  unique() %>%
  knitr::kable()

```

```{r residence_type_distribution, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Residence_type distribution #####
stroke_data %>% 
  ggplot(aes(x = Residence_type, y = ..count.., fill = stroke)) +
  geom_bar() +
  scale_fill_manual(values = c("gray50", "orangered2")) +
  labs(title = "Residence type distribution") +
  theme_bw() +
  theme(legend.position = "bottom")

```

#### 2.2.1.10 Smoking Status

Finally, and as expected, the incidence of strokes is higher in the group of smokers and in those who have ever smoked. However, the incidence is not clearly higher. Where there is an important difference with the base incidence percentage in the "unknown" group. As we will see, in the models that consider smoking_status, "unknown" is used as a decision element.

```{r summary_table_by_smoking_statys, echo=FALSE, message=FALSE}

# __Summary table by smoking_status  #####
# smoking_status, total of observations, number of strokes, percent of strokes
stroke_data %>% 
  group_by(smoking_status) %>%
  summarise(total = n(), percent = round(total/n, 3), strokes = sum(stroke == "stroke"), 
            stroke_percent = round(mean(stroke == "stroke"), 3)) %>% 
  unique() %>%
  knitr::kable()
```

```{r smoking_status_distribution, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Smoking_status distribution #####
stroke_data %>% 
  ggplot(aes(x = smoking_status, y = ..count.., fill = stroke)) +
  geom_bar() +
  scale_fill_manual(values = c("gray50", "orangered2")) +
  labs(title = "Smoking status distribution") +
  theme_bw() +
  theme(legend.position = "bottom")
```

### 2.2.2 Variable importance

The exploratory analysis above allows to get an idea of which variables will have more weight when classifying and predicting the probabilities of suffering a stroke. By the way, it allows us to find those groups with few cases, which might not be present in both data sets (training and test), and which might be worth filtering.

However, and as Rodrigo Amat points out in *Machine Learning with R and caret*, this approach says nothing about the relationship between variables, and their joint effects.

Next we will analyze the correlation between numerical variables, the importance of the variables with the random forest method, and we will make sure that none of the variables have a variance close to zero. Again, it is necessary to refer to Rodrigo Amat as the source of this approach.

#### 2.2.2.1 Correlation between numerical variables

As is well known, a strong correlation between numerical predictors can harm the results of a model. At this point, we find a new stumbling block, in addition to those found previously. Although visually the correlations do not seem strong, numerically they have some importance. In all cases they are less than 0.5, but we cannot say that there is no correlation at all. The values obtained are at that point where we believe that all numerical variables should be kept for modeling purposes, although it might be worth testing by eliminating those that are more correlated in future iterations.

```{r age_vs_avg_glucose_level, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Age vs Avg Glucose Level #####
stroke_data %>% 
  ggplot(aes(age, avg_glucose_level)) +
  geom_point() +
  geom_smooth() +
  labs(title = "Age vs Avg Glucose Level") +
  theme(plot.title = element_text(size = 10, face = "bold")) +
  theme(plot.margin = unit(c(1,0,1,0), "cm"))
```

The numerical correlation between Age and Average Glucose Level is close to 0.24:

```{r age_vs_avg_glucose_level_corr, echo=FALSE, message=FALSE, warning=FALSE}
# __Age vs Avg Glucose Level correlation test #####
cor.test(stroke_data$age, stroke_data$avg_glucose_level, method = "pearson")
```

```{r age_vs_bmi, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Age vs BMI #####
stroke_data %>% 
  ggplot(aes(age, bmi)) +
  geom_point() +
  geom_smooth() +
  labs(title = "Age vs BMI") +
  theme(plot.title = element_text(size = 10, face = "bold")) +
  theme(plot.margin = unit(c(1,0,1,0), "cm"))
```

The correlation between age and body mass index is even slightly stronger, exceeding 0.33.

```{r age_vs_avg_bmi_corr, echo=FALSE, message=FALSE, warning=FALSE}

# __Age vs BMI correlation test
cor.test(stroke_data$age, stroke_data$bmi, method = "pearson")
```

```{r avg_glucose_vs_bmi, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Avg Glucose Level vs BMI #####
stroke_data %>% 
  ggplot(aes(avg_glucose_level, bmi)) +
  geom_point() +
  geom_smooth() +
  labs(title = "Avg Glucose Level vs BMI") +
  theme(plot.title = element_text(size = 10, face = "bold")) +
  theme(plot.margin = unit(c(1,0,1,0), "cm"))
```

Finally, the correlation between Average Glucose Level and Mass Index is around 0.18.

```{r avg_glucose_vs_bmi_corr, echo=FALSE, message=FALSE, warning=FALSE}

# __Avg Glucose Level vs BMI correlation test
cor.test(stroke_data$avg_glucose_level, stroke_data$bmi, method = "pearson")
# 0.1756717
```

#### 2.2.2.1 Contrast of proportions

With respect to categorical variables, Amat's approach is based on performing a portion test. It is about seeing which variables present a proportion of cases that are really different from the baseline level, taking into account the number of cases. The results are ordered from lowest to highest p-value. 

Next we see the table with the first 10 results. Although for some variables it is noted that the Chi-squared approximation may be incorrect (not shown in this table), this exercise confirms part of what we had seen previously: suffering from hypertension or heart disease considerably increases the risk of suffering a stroke.

On the contrary, not having been married, or being a child who does not work, reduces the risk considerably.

Again, we must note that this approach does not take into account the joint effect of the variables, nor the relationship between them (Amat, 2018).

```{r contrast_proportions, echo=FALSE, message=FALSE, warning=FALSE}

# Contrast of proportions #####

# Continuous and qualitative variables that do not group patients are excluded.
stroke_categorical <- stroke_data %>% 
  select(gender, hypertension, heart_disease, ever_married, work_type,
         Residence_type, smoking_status, stroke)

stroke_categorical_tidy <- data.frame(stroke_categorical %>%
                                        gather(key = "variable", value = "group",-stroke))

# An identifier consisting of the name of the variable and the group is added
stroke_categorical_tidy <- stroke_categorical_tidy %>%
  mutate(group_variable = paste(variable, group, sep = "_"))

# Function that calculates the proportions test for the column "Stroke" of a df
proportion_test <- function(df){
  n_strokes <- sum(df$stroke == "stroke") 
  n_no_stroke     <- sum(df$stroke == "no_stroke")
  n_total <- n_strokes + n_no_stroke
  test <- prop.test(x = n_strokes, n = n_total, p = 0.04257486)
  prop_strokes <- n_strokes / n_total
  return(data.frame(p_value = test$p.value, prop_strokes))
}

# The data is grouped by "group_variable" and the test_proportion () function 
# is applied to each group.
prop_analisis <- stroke_categorical_tidy %>%
  group_by(group_variable) %>%
  nest() %>%
  arrange(group_variable) %>%
  mutate(prop_test = map(.x = data, .f = proportion_test)) %>%
  unnest(prop_test) %>%
  arrange(p_value) %>% 
  select(group_variable,p_value, prop_strokes) %>% 
  head(10) 
prop_analisis
```

#### 2.2.2.3 The random forest method

Next, we will review the importance of the variables using the random forest method. It is important to note that this analysis has been carried out on the original, strongly unbalanced dataset. Results will vary when techniques are applied to balance the training set.

This method can be applied in two ways: by accuracy reduction, or Gini reduction. Here we will apply both to see where they coincide and where they differ.

```{r random_forest_method_accuracy, echo=FALSE}

# __Random Forest Method: Accuracy reduction#####
variables_rf <- stroke_data


randforest_model <- randomForest(formula = stroke ~ . ,
                                 data = variables_rf,
                                 mtry = 5,
                                 importance = TRUE, 
                                 ntree = 1000) 

importance <- as.data.frame(randforest_model$importance)
importance <- rownames_to_column(importance,var = "variable")

importance1 <- ggplot(data = importance, aes(x = reorder(variable, MeanDecreaseAccuracy),
                                             y = MeanDecreaseAccuracy,
                                             fill = MeanDecreaseAccuracy)) +
  labs(x = "variable", title = "Accuracy reduction") +
  geom_col() +
  coord_flip() +
  theme_bw() +
  theme(legend.position = "bottom")
importance1
```

```{r random_forest_method_gini, echo=FALSE}

importance2 <- ggplot(data = importance, aes(x = reorder(variable, MeanDecreaseGini),
                                             y = MeanDecreaseGini,
                                             fill = MeanDecreaseGini)) +
  labs(x = "variable", title = "Gini Reduction") +
  geom_col() +
  coord_flip() +
  theme_bw() +
  theme(legend.position = "bottom")
importance2
```

In the case of Accurcy Reduction, the most important variable is Age, followed by Ever Married. BMI and Hypertension are next on the list, but they are not given great weight.

Despite being an unbalanced dataset, this approach yields surprising results to say the least. It is difficult to understand why Ever Married ins prefered over other variables such as BMI or Hypertension.

Gini Reduction shows results more related to what we have seen so far. Age is an important variable in both cases. At the top of the list is Avg Glucose Level and BMI. It is still surprising that Hypertension and Heart Desease carry so little weight in comparison.

If we look more deeply at the case of Heart Disease, we see that its incidence is clearly related to age:

```{r heart_deasead_and_age_table, echo=FALSE, message=FALSE}

# __Summary table by age (rounded nearest 10) and Heart Desease
# age, total of observations, number of strokes, percent of strokes
stroke_data %>% 
  group_by(round_age = round(age, -1)) %>%
  summarise(total = n(), percent = round(total/n, 3), heart_disease = sum(heart_disease == "Yes"),
            heart_desease_percent = round(heart_disease/total,3)) %>% 
  unique() %>%
  knitr::kable()
```

This would explain why, once age is used as a decision factor, Heart Desease loses importance.

The same goes for Hypertension. The incidence, as is obvious, is closely related to age:

```{r hypertension_and_age_table, echo=FALSE, message=FALSE}

# __Summary table by age (rounded nearest 10) and Hypertension
# age, total of observations, number of strokes, percent of strokes
stroke_data %>% 
  group_by(round_age = round(age, -1)) %>%
  summarise(total = n(), percent = round(total/n, 3), hypertension = sum(hypertension == "Yes"),
            hypertension_percent = round(hypertension/total,3)) %>% 
  unique() %>%
  knitr::kable()
```

It is necessary to say again, in any case, that the random forest model did not obtain good results. In this case, the selection of variables effected by this method does not seem the most appropriate, at least for Accuracy Reduction.

#### 2.2.2.3 Near Zero Variance Analysis

The last of the analyzes recommended by Amat, before training models, is the one destined to eliminate the variables that present, before scaling and centering the numerical data, variables close to zero.

Using the original unbalanced dataset, the only variable that presents a variance close to zero is Heart Desease (which would explain its bad positions in the importance analysis carried out previously):

```{r near_zero_variance_analysis, echo=FALSE}

# __Near Zero Variance Analysis #####
stroke_data %>% 
  select(-stroke) %>% 
  nearZeroVar(saveMetrics = TRUE)

```

This changes when techniques are applied to balance the training set, and none of the variables presents variance close to zero.

#### 2.2.2.4 Classification Tree Visualization

Another way to easily visualize important variables is through decision trees. Next we experiment with the unbalanced dataset, aware that it is only a test, and that the final models cannot be trained without first balancing the classes.

The first experiment is to visualize the tree generated by the default "rpart" model. It uses Gini Reduction and applies the following parameters:

1. Complexity parameter = 0.01
2. Minsplit = 20
3. Minbucket = 20/3
4. MaxDepth = 30

As explained in *Learn by marketing*[^4], the complexity parameter (cp) in rpart is the minimum improvement in the model needed at each node. Minsplit is the minimum number of samples at each node, and MaxDepth is the maximum depth of the tree.

As expected, the result is not a tree but a single node that classifies all instances as "no_stroke", given the prevalence of that class:

```{r gini_tree_0.01, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Gini Tree, CP = 0.01, minsplit = 20, minbucket round 20/3, maxdepht = 30 ####
rpart.plot(rpart(stroke ~ ., 
                 data = stroke_data)) # Default rpart tree

```

To see something, we must resort to overtraining. Again, it is important to note that for now we are only experimenting to see if we can see some variables. We are not training any models yet.

To create the following tree we have set the Complexity Parameter to 0.001, and the MaxDeth to 6:

```{r gini_tree_0.001_m_6, echo=FALSE, fig.align='center', fig.width=10, message=FALSE}

# __Gini Tree, CP = 0.001, minslit = 20, minbucket round 20/3, maxdepht = 6 #####
rpart.plot(rpart(stroke ~., 
                 data = stroke_data, 
                 parms=list(split=c("gini")),
                 cp = 0.001,
                 maxdepth = 6))
```

Although this tree requires pruning, we can already see that the variables that are repeated the most are Age, Average Glucose Level and BMI. As for the categorical ones, Heart Desease and Gender appear.

If instead of Gini Reduction we opt for Information Gain, we see that the main numerical variables are Age and Average Glucose Level. With respect to categorical variables, Work Type, Smoking Status and Heart Desease appear.

Both approaches, although we are clearly overtraining, yield results that, intuitively, seem more accurate than those obtained with the random forest method.

The following tree uses the Information Gain method, the CP is set to 0.0023, and the MaxDepth is equal to 6:

```{r information_tree_0.0023_m_6, echo=FALSE, fig.align='center', fig.width=10, message=FALSE}

# __Information Tree, CP = 0.0023. minslit = 20, minbucket round 20/3, maxdepht = 6 #####
rpart.plot(rpart(stroke ~., 
                 data = stroke_data, 
                 parms=list(split=c("information")),
                 cp = 0.0023,
                 maxdepth = 6))

```

It is interesting to note that if we try to create a tree only using categorical variables, no results are obtained with any method. This may be due to the fact that these types of variables are not capable of classifying between "stroke" and "no_stroke" by themselves, and that they need to interact with numeric variables.

The latter, on the other hand, are capable of generating trees when they interact with each other, without the need for categorical variables.

#### 2.2.2.5 Balanced Data

But what happens if we balance the data? For this exercise, we have used three methods to match the number of instances classified as "stroke" and "no_stroke", using the "ROSE" library.

1. Oversamplig: observations from the minority class are randomly added.
2. Both: the distributions are equalized by randomly adding observations from the minority class, and randomly removing observations from the majority class.
3. Better estimates: the library has a function (ROSE) that generates synthetic data to balance the classes. *The data generated using ROSE is considered to provide better estimate of original data*[^5].

Next we review, for the case of Oversamplig, that although the classes have been balanced, the statistics and the distribution of the variables are maintained. This is true for the other two methods used.

After applying the "ovun.sample" function, with the "over" method, the new distribution is as follows:

```{r over_sampling, echo=FALSE, message=FALSE, warning=FALSE}

# _______________________########
# Over Sampling ######
# _______________________########

n_over = sum(stroke_data == "no_stroke")

set.seed(1969, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1969)`

stroke_data_over <- ovun.sample(stroke ~ ., data = stroke_data, method = "over", N = n_over*2)$data

# Relevel "stroke" "no_stroke" factors: positive class: "stroke"
stroke_data_over$stroke <- relevel(stroke_data_over$stroke, ref = "stroke")

table(stroke_data_over$stroke) %>% 
  kable()

prop.table(table(stroke_data_over$stroke))  %>% 
  kable()
```

This, as has been said, does not change the statistics or the distributions of the numerical variables. Below for the Age variable, we show the original tables and graphs, compared with balanced data using the oversampling method.

*Original data*

```{r stat_age_data_2_orig, echo=FALSE, message=FALSE}

# __Statistical Age Data  ####
stroke_data %>% 
  group_by(stroke) %>% 
  summarise(avg_age = round(mean(age),1),
            median_age = round(median(age)),
            min_age = min(age),
            max_age = max(age)) %>% 
  kable()
```

```{r age_density_plot_2, echo=FALSE, fig.align='center', fig.width=10, message=FALSE}

# __Density Plot: Avg Level per Class ####
stroke_data %>% ggplot(aes(age, fill = stroke)) +
  geom_density(alpha = 0.2, bw = 1) +
  labs(title = "Age density plot - Original") +
  theme(plot.title = element_text(size = 12, face = "bold")) +
  theme(plot.margin = unit(c(1,0,1,0), "cm"))

```

*Balanced data (oversampling method)*

```{r stat_age_data_blanced_over, echo=FALSE, message=FALSE}

# __Statistical Age Data  #####

stroke_data_over %>% 
  group_by(stroke) %>% 
  summarise(avg_age = round(mean(age),1),
            median_age = round(median(age)),
            min_age = min(age),
            max_age = max(age)) %>% 
  kable()
```

```{r age_density_plot_balnaced, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# __Density Plot: Age per Class ####
stroke_data_over %>% ggplot(aes(age, fill = stroke)) +
  geom_density(alpha = 0.2, bw = 1) +
  labs(title = "Age density plot - Balanced") +
  theme(plot.title = element_text(size = 12, face = "bold")) +
  theme(plot.margin = unit(c(1,0,1,0), "cm"))
```

In the case of categorical variables, similar proportions of the groups are maintained, but the percentage of "strokes" changes. What used to be 0.6% becomes 60%.

To exemplify the case of categorical variables, we have selected Smoking Status:

*Original data*

```{r summ_smoking_2, echo=FALSE, message=FALSE}
# __Summary table by smoking_status  #####
# smoking_status, total of observations, number of strokes, percent of strokes
stroke_data %>% 
  group_by(smoking_status) %>%
  summarise(total = n(), percent = round(total/n, 3), strokes = sum(stroke == "stroke"), 
            stroke_percent = round(mean(stroke == "stroke"), 3)) %>% 
  unique() %>%
  knitr::kable()
```

*Balanced data (oversampling method)*

```{r summ_smoking_over, echo=FALSE, message=FALSE}
# __Summary table by smoking_status  #####
# smoking_status, total of observations, number of strokes, percent of strokes

n_stroke_over <-  nrow(stroke_data_over)
  
stroke_data_over %>% 
  group_by(smoking_status) %>%
  summarise(total = n(), percent = round(total/n_stroke_over, 3), strokes = sum(stroke == "stroke"), 
            stroke_percent = round(mean(stroke == "stroke"), 3)) %>% 
  unique() %>%
  knitr::kable()
```

This same check has been carried out for all methods and all variables (numerical and categorical ones).

We have also verified that the correlation between numerical variables is similar to that found with the original data.

So what changes? With the data balanced using the oversampling method, we see changes in the list of variables obtained with the contrast of proportions (here we show the first ten), and with the random forest method. This is precisely what we are looking for: determining the importance of variables without the prevalence problems of the original dataset.

```{r contrast_prop_2_over, echo=FALSE, message=FALSE, warning=FALSE}
# __Contrast of proportions #####

# Continuous and qualitative variables that do not group patients are excluded.
stroke_categorical_over <- stroke_data_over %>% 
  filter(!gender == "Other") %>% 
  select(gender, hypertension, heart_disease, ever_married, work_type,
         Residence_type, smoking_status, stroke)

stroke_categorical_tidy_over <- data.frame(stroke_categorical_over %>%
                                             gather(key = "variable", value = "group",-stroke))

# An identifier consisting of the name of the variable and the group is addedo 
stroke_categorical_tidy_over <- stroke_categorical_tidy_over %>%
  mutate(group_variable = paste(variable, group, sep = "_"))

# Function that calculates the proportions test for the column "Stroke" of a df
proportion_test_over <- function(df){
  n_strokes <- sum(df$stroke == "stroke") 
  n_no_stroke     <- sum(df$stroke == "no_stroke")
  n_total <- n_strokes + n_no_stroke
  test <- prop.test(x = n_strokes, n = n_total, p = 0.5)
  prop_strokes <- n_strokes / n_total
  return(data.frame(p_value = test$p.value, prop_strokes))
}

# The data is grouped by "group_variable" and the test_proportion () function 
# is applied to each group.
prop_analisis_over <- stroke_categorical_tidy_over %>%
  group_by(group_variable) %>%
  nest() %>%
  arrange(group_variable) %>%
  mutate(prop_test = map(.x = data, .f = proportion_test_over)) %>%
  unnest(prop_test) %>%
  arrange(p_value) %>% 
  select(group_variable,p_value, prop_strokes) %>% 
  head(10) 
prop_analisis_over
```

```{r random_forest_method_accuracy_over, echo=FALSE}

# __Random Forest Method #####
variables_rf_over <- stroke_data_over


randforest_model_over <- randomForest(formula = stroke ~ . ,
                                      data = variables_rf_over,
                                      mtry = 5,
                                      importance = TRUE, 
                                      ntree = 1000) 

importance_over <- as.data.frame(randforest_model_over$importance)
importance_over <- rownames_to_column(importance_over,var = "variable")

importance1_over <- ggplot(data = importance_over, aes(x = reorder(variable, MeanDecreaseAccuracy),
                                                       y = MeanDecreaseAccuracy,
                                                       fill = MeanDecreaseAccuracy)) +
  labs(x = "variable", title = "Accuracy reduction - Oversampling method") +
  geom_col() +
  coord_flip() +
  theme_bw() +
  theme(legend.position = "bottom")
importance1_over

```

```{r random_forest_method_gini_over, echo=FALSE}

importance2_over <- ggplot(data = importance_over, aes(x = reorder(variable, MeanDecreaseGini),
                                                       y = MeanDecreaseGini,
                                                       fill = MeanDecreaseGini)) +
  labs(x = "variable", title = "Gini Reduction - Oversamplig method") +
  geom_col() +
  coord_flip() +
  theme_bw() +
  theme(legend.position = "bottom")
importance2_over
```

Finally, we have verified that the balanced data do not present variables with variation close to zero. Again, we show the results for the balanced data with the oversampling method:

```{r near_zero_variance_over, echo=FALSE, message=FALSE}
# __Near Zero Variance Analysis #####
stroke_data_over %>% 
  select(-stroke) %>% 
  nearZeroVar(saveMetrics = TRUE)
```

These outputs are similar for all the methods used to balance the data, although their small differences cause different results in the applied models, as we will in the next chapters.

To illustrate the differences between the results obtained with the different methods to balance the data, we show the trees that are now generated with the default "rpart" algorithm.

```{r both_sampling, message=FALSE, warning=FALSE, include=FALSE}

# _______________________########
# Over_Under Sampling: Both ######
# _______________________########
n_both = sum(stroke_data$stroke == "stroke") + sum(stroke_data$stroke == "no_stroke")

set.seed(1969, sample.kind="Rounding") # Every time we run the code, we get a different ovun.sample
# Accuracy and balanced accuracy strongly depends on this ramdom process.

stroke_data_both <- ovun.sample(stroke ~ ., data = stroke_data, method = "both", p = 0.5, N = n_both)$data
table(stroke_data_both$stroke)

# Relevel "stroke" "no_stroke" factors: positive class: "stroke"
stroke_data_both$stroke <- relevel(stroke_data_both$stroke, ref = "stroke")
```

```{r better_sampling, message=FALSE, warning=FALSE, include=FALSE}

# _______________________########
# Better Estimates #####
# _______________________########

table(stroke_data$stroke)
prop.table(table(stroke_data$stroke))

set.seed(1969, sample.kind="Rounding") # Every time we run the code, we get a different ovun.sample
# Accuracy and balanced accuracy strongly depends on this ramdom process.

stroke_data_better <- ROSE(stroke ~ ., data = stroke_data)$data
table(stroke_data_better$stroke)

# Relevel "stroke" "no_stroke" factors: positive class: "stroke"
stroke_data_better$stroke <- relevel(stroke_data_better$stroke, ref = "stroke")
```

***

**Oversampling**

```{r defaul_rpart_tree_over, echo=FALSE, fig.align='center', fig.width=15, message=FALSE, warning=FALSE}

# Gini Tree, CP = 0.01, minslit = 20, minbucket round 20/3, maxdepht = 30 ####
rpart.plot(rpart(stroke ~ ., 
                 data = stroke_data_over)) # Default rpart tree
```

***

**Both**

```{r defaul_rpart_tree_both, echo=FALSE, fig.align='center', fig.width=15, message=FALSE, warning=FALSE}

# Gini Tree, CP = 0.01, minslit = 20, minbucket round 20/3, maxdepht = 30 ####
rpart.plot(rpart(stroke ~ ., 
                 data = stroke_data_both)) # Default rpart tree
```

***

**Better Estimates**

```{r defaul_rpart_tree_better, echo=FALSE, fig.align='center', fig.width=15, message=FALSE, warning=FALSE}

# Gini Tree, CP = 0.01, minslit = 20, minbucket round 20/3, maxdepht = 30 ####
rpart.plot(rpart(stroke ~ ., 
                 data = stroke_data_better)) # Default rpart tree
```

***
As we can see, "Age" is the most important variable in all cases. Average Glucose Level" is also common in trees generated by all three methods.

The "oversampling" and "both" methods generate trees in which smoking status is relevant. "BMI" only appears when using the "Both" method. Better Estimates only takes into account the variables "Age" and "Average Glucose Level". Finally, the tree generated with the oversampling method incorporates the variable "Gender" as a decision element.

It is important to note that the processes for balancing the data are random. Every time we apply the function by selecting the "Oversampling" and "Both" methods, we obtain different datasets, and this can cause certain variables to acquire more or less prominence.

### 2.2.3 Summary of Exploratory Analysis

At this point, we know that:

1. The original dataset is not large. We have just over 5,000 observations.
2. The original data is strongly unbalanced. Only 4% of the observations (about 200) are classified as "stroke".
3. There are no variables that completely separate the two classes. The numerical variables overlap, and the categorical variables are present in both classes (some of them with balanced distributions). This means that predictors with very similar values are present in observations classified as "stroke" and "no_stroke".
4. The numerical variables (Age, Average Glucose Level and BMI), which are important according to various analysis techniques, show a certain correlation.
5. Variables that present a clearly different incidence from the baseline, such as Heart Desease and Hypertension, do not seem to be important when analyzed together with the rest of the variables. Their relationship with Age is so strong that once Age is selected as the main variable, the others lose weight.

The situation, therefore, does not look very promising when it comes to finding a model that yields good results. Without having trained any model yet, we can already anticipate that algorithms based on neighbors, for example, should not be suitable in this case.

## 2.3 Modeling Approach

### 2.3.1 Train and test sets

As anticipated in the introduction, for the creation of the training set we reserve 80% of the data. Because the positive class "stroke" is very minor, before training the models we made sure that both classes were present in the test set, and that they had a similar distribution.

```{r train_test_sets, message=FALSE, include=FALSE}

# Creating train and test set #####

# We will use 80% of data to train, and 20% of data to test.
set.seed(1970, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1970)`
test_index <- createDataPartition(y = stroke_data$stroke, times = 1, p = 0.2,
                                  list = FALSE)

# __train_stroke ####
train_stroke <- stroke_data[-test_index,]
str(train_stroke)

# __test_stroke ####
test_stroke <- stroke_data[test_index,]
str(test_stroke)
```

```{r review_proportions, echo=TRUE}

# Review #####
# Comparing test and train set to be sure that proportions are similar

# __Percent of strokes in train set ####
mean(train_stroke$stroke == "stroke") 


# __Percent of strokes in test set ####
mean(test_stroke$stroke == "stroke") 

```

A similar analysis was done for the rest of the variables. Only some important distribution differences were found in the case of Average_Glucose_Level.

### 2.3.2 Cross Validation and Criteria to Select the Best Tuning Parameters

Before continuing, we show here the selected cross-validation parameters to train all the models of the caret package:

```{r train_control, echo=TRUE}

# _______________________########
# TRAIN CONTOL  ########
# For Caret Trains ########
# _______________________########

ctrl <- trainControl(method="repeatedcv", 
                     number = 10,
                     repeats = 5, summaryFunction=twoClassSummary, classProbs=T,
                     savePredictions = T) 
# Parameters ClassProbs and SavePred: needed to plot ROC Curves.

```

We should also mention that the criterion to obtain the best parameters was in all cases the **Area Under the ROC Curve**.

It is clear that in this case, Accuracy is not useful for determining the quality of the models, due to the prevalence of the negative class. If the model predicts "no_stroke" in all cases, it will be 96% accurate.

Furthermore, this is the typical situation where sensitivity is more important than specificity. A false positive is preferable to a false negative. However, a model that predicts "stroke" in all cases is also not useful.

A high ROC, together with other indicators, is usually obtained when there is a certain balance between sensitivity and specificity (except in some exceptions), and this is what we wanted to achieve in the development of this project. Always giving priority to sensitivity, we wanted to maintain specificity at values above 70%.

### 2.3.3 Training Models

Despite the comments in point 5 of the *Summary of Exploratory Analysis section*, all the variables contribute to positive reductions (in Accuracy and Gini). This, together with the fact that depending on the balancing method some variables acquire more importance than others, has made us believe that it is worth training and comparing models taking into account all the variables. We will let the models capable of doing so select the predictor variables through cross validation.

Given the nature of this project, we think the main goal is to experiment with various models. This allows us to analyze their differences, not only in terms of results, but also with respect to other aspects, such as computation times, tune parameters, and explicability.

In this report we will focus on the models that allow us to exemplify the process, whether it be by contrast between results, or by their quality.

As part of this experimentation, the first step was to train a series of models with the original data (after scaling and centralizing the numerical variables). At this stage, we were fully aware that due to the prevalence of the negative class, the results were not going to be good.

```{r prepocessing, message=FALSE, include=FALSE}

# PreProcessing: Centering and Scaling numerical variables ######
#(t stands for transformed)#####

preProcValues <- preProcess(train_stroke, method = c("center", "scale"))

# __train_stroke_t ####
train_stroke_t <- predict(preProcValues, train_stroke)

# __test_stroke_t ####
test_stroke_t <- predict(preProcValues, test_stroke)
```

With the original umbalnced data, the following models were trained:

- Rpart native
    - Gini, CP = 0.01, minslit = 20, minbucket round 20/3, maxdepht = 30 (default)
    - Gini Tree, CP = 0.001, minslit = 20, minbucket round 20/3, maxdepht = 30
    - Gini Tree, CP = 0.0024, minslit = 20, minbucket round 20/3, maxdepht = 30
    - Gini Tree, CP = 0.001, minslit = 20, minbucket round 20/3, maxdepht = 5
    - Information Tree, CP = 0.01, minslit = 20, minbucket round 20/3, maxdepht = 30
    - Information Tree, CP = 0.001. minslit = 20, minbucket round 20/3, maxdepht = 30
    - Information Tree, CP = 0.0023. minslit = 20, minbucket round 20/3, maxdepht = 30
- Default Caret Rpart 
- K-Nearest-Neighbor (KNN)
- Random Forest (RF)
- Flexible Discriminant Analysis (FDA)

As objectives, this previous exercise had:

1. For classification trees, test complexity and pruning parameters by hand to see what each one does.
2. Compare the performance of the "rpart" method of the caret package with the tree generated by the default rpart function.
3. Check the results of three classification models, two of them seen in the course documentation (KNN and RF), and another not included in it (FDA).

Once we had verified what we already knew (with the unbalanced data we were not going to obtain correct results), we balanced the training set with the methods already described in the previous section, and we trained the following models for each of the cases (oversampling, both and better estimates):

- RPART native
    - Gini Tree, CP = 0.01, minslit = 20, minbucket round 20/3, maxdepht = 30
    - Gini Tree, CP = 0.001, minslit = 20, minbucket round 20/3, maxdepht = 30
    - Default Gini Tree & Cost Matrix 3 to 1
- RPART caret
- K-Nearest-Neighbor (KNN)
- Random Forest (RF)
- Neural Network (NNET)
- Flexible Discriminant Analysis (FDA)
- Naives Bayes (NB)

La selección de estos modelos  para el caso de datos balanceados responde a razones similares a las expuestas para el caso de los datos originales. 

Once the aforementioned models were trained, for each balancing method, we were able to determine which model, by itself, yields the best results.

However, to see if we could further improve those results, we created two Ensambles with the best-performing models for each balancing method.

The criteria for the first Ensamble was the "Area Under de Roc Curve" (AUC). For the second Ensamble we opted for "Balanced Accuracy" to select the models.

Once the Ensambled were created, we selected "Balaced Acccuracy" as the comparison metric in both cases, and we looked for the feedback criteria that optimized that metric (if x or more models predict "stroke", then predict "stroke").

# 3. Results

## 3.1 Unbalanced data

We are not going to stop long here, because as expected the results are not good. With native rpart models, overtraining is required to obtain a sensitivity of just 2% (1 correct classification of 42). With a Complexity Parameter (CP) set to 0.01, the model is not able to classify any instance as "stroke" (with an Accuracy of close to 96%, which is the prevalence of the negative class):

### 3.1.1 Gini, CP = 0.01, minslit = 20, minbucket round 20/3, maxdepht = 30

```{r gini_001_20_30_unb, echo=FALSE, message=FALSE, warning=FALSE}

# Gini, CP = 0.01, minslit = 20, minbucket round 20/3, maxdepht = 30 ####
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`

gini_tree_cp0.01 <- rpart(stroke ~ ., 
                          data = train_stroke_t) # Default rpart tree

y_hat_gini_tree_cp0.01 <- predict(gini_tree_cp0.01, test_stroke_t, type = "class")

# Model Evaluation :::::::

# Confusion Matrix
cm_gini_tree_cp0.01 <- confusionMatrix(y_hat_gini_tree_cp0.01, test_stroke$stroke)
cm_gini_tree_cp0.01
```

This is true for all models based on the native rpart function. If we set the CP to below 0.0023, then 1 instance is classified as "stroke". We are still far from optimal results. Below we show the results of the Gini-based model, with a CP of 0.001:

### 3.1.2 Gini Tree, CP = 0.001, minslit = 20, minbucket round 20/3, maxdepht = 30

```{r gini_0001_20_30_unb, echo=FALSE, message=FALSE, warning=FALSE}

# Gini Tree, CP = 0.001, minslit = 20, minbucket round 20/3, maxdepht = 30 #####
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
gini_tree_cp0.001 <- rpart(stroke ~., 
                           data = train_stroke_t, 
                           parms=list(split=c("gini")),
                           cp = 0.001)

y_hat_gini_tree_cp0.001 <- predict(gini_tree_cp0.001, test_stroke_t, type = "class")

# Model Evaluation :::::::

# Confusion Matrix
cm_y_hat_gini_tree_cp0.001 <- confusionMatrix(y_hat_gini_tree_cp0.001, 
                                              test_stroke_t$stroke)
cm_y_hat_gini_tree_cp0.001
```

These results are similar for most of the cases in which the CP is less than 0.0023

The results of the "train" function (caret package), with the "rpart" method are practically the same.

### 3.1.3 RPART caret, KNN and RF

```{r rpart_caret_unb, echo=FALSE, message=FALSE, warning=FALSE}
# RPART caret ######
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
train_caret_tree <- train(stroke ~ ., method = "rpart", 
                          data = train_stroke_t, 
                          trControl = ctrl, 
                          metric = "ROC")

y_hat_caret_tree <- predict(train_caret_tree, test_stroke_t)

# Model Evaluation ::::::::

cm_caret_tree <- confusionMatrix(y_hat_caret_tree, test_stroke_t$stroke)
cm_caret_tree
```

In this case, the CP selected by cross validation, and which optimizes the AUC, is 0.0015:

```{r cp_train_caret_rpart_umb, echo=FALSE, fig.width=10, message=FALSE, warning=FALSE}

plot(train_caret_tree, 
     main = "RPART caret CP Parameters")

```

As for the other models, KNN does not classify any instance as "stroke", RF gives results similar to those seen for the overtrained decision trees (a sensitivity of around 2%).

### 3.1.4 Flexible Discriminant Analysis (FDA)

The only model that stands out in this phase, comparatively speaking, is FDA. This model is able to correctly classify as "stroke" 14% of the instances (6 of 42):

```{r fda_unb, echo=FALSE, message=FALSE, warning=FALSE}

# FDA #####
# Flexible Discriminant Analysis #####
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
train_fda <- train(stroke ~ ., method = "fda", 
                   data = train_stroke_t, 
                   trControl = ctrl, 
                   metric = "ROC")

y_hat_fda <- predict(train_fda, test_stroke_t)

# Model Evaluation ::::::::

cm_fda <- confusionMatrix(y_hat_fda, test_stroke_t$stroke)
cm_fda
```

The results are still deplorable, but this model already makes a difference with the others.

## 3.2 Balanced data

```{r oversamplig_training_set, message=FALSE, warning=FALSE, include=FALSE}

# _______________________######## 
# Oversampling #####
# _______________________######## 

table(train_stroke_t$stroke)
prop.table(table(train_stroke_t$stroke))

n_over = sum(train_stroke_t$stroke == "no_stroke")

set.seed(1969, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1969)`
# Every time we run the code, we get a different ovun.sample
# Visualization tree, Accuracy, Sensibility, Balanced accuracy and F.meas strongly depends on this ramdom process.

train_stroke_over <- ovun.sample(stroke ~ ., data = train_stroke_t, method = "over", N = n_over*2)$data
table(train_stroke_over$stroke)

# Relevel "stroke" "no_stroke" factors: positive class: "stroke" #### 
train_stroke_over$stroke <- relevel(train_stroke_over$stroke, ref = "stroke")
```

It is time to get serious and train models after solving one of the main obstacles: the imbalance of the data.

This, as we will see, improves the metrics, but we cannot expect outstanding results (a high Accuracy, along with high Sensitivity and Specificity). This, as we saw in the exploratory section, does not seem possible, since there are no predictor variables capable of clearly separating the classes.

In our approach, we have preferred to sacrifice Accuracy in favor of Sensitivity. At the same time, as already mentioned, we wanted to keep the Specificity above 70%.

It is important to say that for modeling purposes, the balancing methods have been applied only on the scaled and centralized training set. The test set has remained unbalanced.

### 3.2.1 Oversamplig Method

For this and the other methods to generate a balanced training set, the steps described in *Practical Guide to deal with Imbalanced Classification Problems in R* have been followed.

We will now review the results of the different models trained with the set generated by oversampling.

#### 3.2.1.1 Gini Tree, CP = 0.01, minslit = 20, minbucket round 20/3, maxdepht = 30

```{r gini_001_20_30_over, echo=FALSE, message=FALSE, warning=FALSE}

# RPART native #######
# Recursive Partitioning and Regression Trees ######

# Gini Tree, CP = 0.01, minslit = 20, minbucket round 20/3, maxdepht = 30 ####
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
gini_tree_cp0.01_over <- rpart(stroke ~ ., 
                               data = train_stroke_over)

y_hat_gini_tree_cp0.01_over <- predict(gini_tree_cp0.01_over, test_stroke_t, 
                                       type = "class")
# Model Evaluation ::::::::

cm_gini_tree_cp0.01_over <- confusionMatrix(y_hat_gini_tree_cp0.01_over, 
                                            test_stroke_t$stroke)
cm_gini_tree_cp0.01_over
```

This model has an Accuracy close to 76%. Its Sensitivity is around 74%, and its Specificity is 76%. The Balanced Accuracy is close to 75%. They are much better results than the previous ones. However, for the type of problem in question, the sensitivity is low. It has wrongly classified as negative 11 of 42 cases. The area under the ROC curve is around 0.74.

```{r sens_espec_ROC_gini_001_20_30_over, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# Calc. Probs for every class
roc_gini_tree_cp0.01_over <- predict(gini_tree_cp0.01_over,
                                     test_stroke_t, type = "prob") %>% 
  data.frame()

# Sensitivity vs Specificity ROC CURVE
sens_roc_gini_tree_cp0.01_over <- plot(roc(response = test_stroke_t$stroke, 
                                           predictor = roc_gini_tree_cp0.01_over$stroke, levels = c("stroke", "no_stroke")), 
                                       print.auc = TRUE,
                                       xlim = c(1,0),
                                       ylim = c(0,1),
                                       xlab = "Specificity", 
                                       ylab ="Sensibility", 
                                       main = "Gini Tree, CP: 0.01 - Over")
```

To obtain these results, the model has selected as best tunes a CP of 0.011, and 8 nodes:

```{r plot_gini_tree_cp0.01_over, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

plotcp(gini_tree_cp0.01_over)

```

From a CP of 0.015 the improvement is minimal.

The variables considered have been:

*Variable importance*

|age|ever_married|work_type|bmi|smoking_status|avg_glucose_level|gender|
|---:|----------:|--------:|---:|------------:|----------------:|-----:|
|52  |         14|        9|  9|             8|                7|     1|

#### 3.2.1.2 Gini Tree, CP = 0.001, minslit = 20, minbucket round 20/3, maxdepht = 3

With balanced data, overtraining does not give good results in terms of Sensitivity. Although we get better Accuracy, the sensitivity drops to 48%, and the Balanced Accuracy does not reach 70%:

```{r gini_0001_20_30, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# Gini Tree, CP = 0.001, minslit = 20, minbucket round 20/3, maxdepht = 30 #####
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
gini_tree_cp0.001_over <- rpart(stroke ~., 
                                data = train_stroke_over, 
                                parms=list(split=c("gini")),
                                cp = 0.001)

y_hat_gini_tree_cp0.001_over <- predict(gini_tree_cp0.001_over, test_stroke_t, 
                                        type = "class")

# Model Evaluation ::::::::

cm_gini_tree_cp0.001_over <- confusionMatrix(y_hat_gini_tree_cp0.001_over, 
                                             test_stroke_t$stroke)
cm_gini_tree_cp0.001_over
```

The area under the curve (bellow 0.5) tells us that the model is clearly deficient:

```{r sens_espec_ROC_gini_0001_20_30_over, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# Calc. Probs for every class
roc_gini_tree_cp0.001_over <- predict(gini_tree_cp0.001_over,
                                      test_stroke_t, type = "prob") %>% 
  data.frame()

# Sensitivity vs Specificity ROC CURVE
sens_espec_roc_gini_tree_cp0.001_over <- plot(roc(response = test_stroke_t$stroke, 
                                                  predictor = roc_gini_tree_cp0.001_over$stroke, levels = c("stroke", "no_stroke")), 
                                              print.auc = TRUE,
                                              xlim = c(1,0),
                                              ylim = c(0,1),
                                              xlab = "Specificity", 
                                              ylab ="Sensibility", 
                                              main = "Gini Tree, CP:001 - Over")

# Sensitivity vs Specificity AUC
auc(sens_espec_roc_gini_tree_cp0.001_over)
```

In this case, overtraining is evident if we look at the size of the tree, with 109 modules:

```{r plot_gini_tree_cp0.001_over, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

plotcp(gini_tree_cp0.001_over)
```

#### 3.2.1.3 Default Gini Tree & Cost Matrix 3 to 1

In this model, we have experimented with a cost matrix, giving preference to Sensitivity (a false negative is 3 times worse than a false positive). If we increase the cost, we gain Sensitivity, but we lose Balanced Accuracy):

```{r gini_tree_cost_over, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# Default Gini Tree & Cost Matrix 3 to 1 #####
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
cost_matrix_tree_over <- rpart(stroke ~ ., 
                               data = train_stroke_over,
                               parms=list(
                                 loss=matrix(c(0,3,1,0), # A false negative is 3 times worse than a false positive
                                             byrow=TRUE,
                                             nrow=2)))

y_hat_cost_matrix_tree_over <- predict(cost_matrix_tree_over, test_stroke_t, 
                                       type = "class")

# Model Evaluation ::::::::

cm_cost_matrix_tree_over <- confusionMatrix(y_hat_cost_matrix_tree_over, 
                                            test_stroke_t$stroke)
cm_cost_matrix_tree_over
```

With this model, the Sensitivity rises to 90%, but the Specificity barely reaches 63%.

The area under the ROC curve exceeds 0.75.

```{r sens_espec_ROC_gini_tree_cost_over, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# Calc. Probs for every class
roc_cost_matrix_tree_over <- predict(cost_matrix_tree_over,
                                     test_stroke_t, type = "prob") %>% 
  data.frame()

# Sensitivity vs Specificity ROC CURVE
sens_espec_roc_cost_matrix_tree_over <- plot(roc(response = test_stroke_t$stroke, 
                                                 predictor = roc_cost_matrix_tree_over$stroke, levels = c("stroke", "no_stroke")), 
                                             print.auc = TRUE,
                                             xlim = c(1,0),
                                             ylim = c(0,1),
                                             xlab = "Specificity", 
                                             ylab ="Sensibility", 
                                             main = "Default Gini Tree & Cost Matrix 3 to 1 - Over")

# Sensitivity vs Specificity AUC
auc(sens_espec_roc_cost_matrix_tree_over)
```

The CP is 0.01, as specified, and the tree size is reasonable:

```{r plot_gini_tree_cost_over, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

plotcp(cost_matrix_tree_over)
```

The variables considered have been:

*Variable importance*

|age|bmi|work_type|ever_married|work_type |avg_glucose_level|gender|
|--:|--:|--------:|-----------:|---------:|----------------:|-----:|
|46 | 18|       14|           9|        13|                4|     2|

Smoking Status and Hypertension also appear, but with less importance.

#### 3.2.1.4 RPART caret

As you might expect, the results of this model are similar to those of native default rpart (Gini Tree, CP = 0.01, minslit = 20, minbucket round 20/3, maxdepht = 30).

```{r rpart_caret_over, echo=FALSE, message=FALSE, warning=FALSE}

# RPART caret ####
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
train_caret_tree_over <- train(stroke ~ ., method = "rpart",
                               data = train_stroke_over,
                               trControl = ctrl,
                               metric="ROC")

y_hat_caret_tree_over <- predict(train_caret_tree_over, test_stroke_t)

# Model Evaluation ::::::::

cm_caret_tree_over <- confusionMatrix(y_hat_caret_tree_over, test_stroke_t$stroke)
cm_caret_tree_over
```

With a Sensitivity of around 74%, and a Specificity of 78%, this is the best model achieved so far. The Balanced Accuracy is around 76%. 

The area under the ROC curve reaches 0.8:

```{r sens_espec_ROC_caret_tree_over, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# Calc. Probs for every class
roc_caret_tree_over <- predict(train_caret_tree_over,
                               test_stroke_t, type = "prob")

# Sensitivity vs Specificity ROC CURVE
sens_espec_roc_caret_tree_over <- plot(roc(response = test_stroke_t$stroke, 
                                           predictor = roc_caret_tree_over$stroke, levels = c("stroke", "no_stroke")), 
                                       print.auc = TRUE,
                                       xlim = c(1,0),
                                       ylim = c(0,1),
                                       xlab = "Specificity", 
                                       ylab ="Sensibility", 
                                       main = "RPART caret - Over")

# Sensitivity vs Specificity AUC
auc(sens_espec_roc_caret_tree_over)
```

The final CP was 0.01729183, and the variables considered were:

*Variable importance*

|age|ever_marriedYes|bmi      |smoking_statusUnknown|avg_glucose_level|avg_glucose_level|smoking_statussmokes|
|--:|--------------:|--------:|--------------------:|----------------:|----------------:|-------------------:|
|61 |             18|        8|                    6|                6|                6|                   1|

#### 3.2.1.5 K-Nearest-Neighbor

As we anticipated in the exploratory section, a model based on the nearest neighbors is not suitable in this case.

```{r knn_over, echo=FALSE, message=FALSE, warning=FALSE}

# K-Nearest-Neighbor #######
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
train_knn_over <- train(stroke ~ ., method = "knn", 
                        data = train_stroke_over, 
                        trControl = ctrl,
                        metric="ROC")

y_hat_knn_over <- predict(train_knn_over, test_stroke_t)

# Model Evaluation ::::::::

cm_knn_over <- confusionMatrix(y_hat_knn_over, test_stroke_t$stroke)
cm_knn_over
```

Although the Accuracy exceeds 76% the Sensitivity only touches 43%. With a Specificity of 78%, the Balanced Accuracy just exceeds 60%

The area under the ROC curve is less than 0.5: 

```{r sens_espec_ROC_knn_over, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# Calc. Probs for every class
roc_knn_over <- predict(train_knn_over,
                        test_stroke_t, type = "prob")

# Sensitivity vs Specificity ROC CURVE
sens_espec_roc_knn_over <- plot(roc(response = test_stroke_t$stroke, 
                                    predictor = roc_knn_over$stroke, levels = c("stroke", "no_stroke")), 
                                print.auc = TRUE,
                                xlim = c(1,0),
                                ylim = c(0,1),
                                xlab = "Specificity", 
                                ylab ="Sensibility", 
                                main = "KNN - Over")

# Sensitivity vs Specificity AUC
auc(sens_espec_roc_knn_over)
```

In this case, the best tune for k = 9.

#### 3.2.1.6 Ramdom Forest

This model requires the longest calculation time. Despite that, it is one of the worst performers in terms of Sensitivity and Balanced Accuracy.

```{r rf_over, echo=FALSE, message=FALSE, warning=FALSE}

# RANDOM FOREST ########
# it takes time! ######
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
train_rf_over <- train(stroke ~ ., method = "rf", 
                       data = train_stroke_over, 
                       trControl = ctrl,
                       metric="ROC")

y_hat_rf_over <- predict(train_rf_over, test_stroke_t, type = "raw")

# Model Evaluation ::::::::

cm_rf_over <- confusionMatrix(y_hat_rf_over, test_stroke_t$stroke)
cm_rf_over
```

The sensitivity of this model barely reaches 7%. Surprisingly, it has an area under the ROC curve of 0.78 (something we must analyze in future steps to fully understand).

```{r sens_espec_ROC_rf, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# Calc. Probs for every class
roc_rf_over <- predict(train_rf_over,
                       test_stroke_t, type = "prob")

# Sensitivity vs Specificity ROC CURVE
sens_espec_roc_rf_over <- plot(roc(response = test_stroke_t$stroke, 
                                   predictor = roc_rf_over$stroke, levels = c("stroke", "no_stroke")), 
                               print.auc = TRUE,
                               xlim = c(1,0),
                               ylim = c(0,1),
                               xlab = "Specificity", 
                               ylab ="Sensibility", 
                               main = "Random Forest - Over")

# Sensitivity vs Specificity ROC CURVE
auc(sens_espec_roc_rf_over)
```

In this case, the best tune for "mtry" parameter (randomly selected predictors) was 9.

#### 3.2.1.7 Neural Network

This is another model with a comparatively good performance. 

```{r nnet_over, echo=FALSE, message=FALSE, warning=FALSE}

# NNET: great variability !!!#####
# Neural Network ####
# It takes some time!

set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
train_nnet_over <- train(stroke ~ ., method = "nnet",
                         data = train_stroke_over,
                         trControl = ctrl,
                         trace = FALSE,
                         metric="ROC")

y_hat_nnet_over <- predict(train_nnet_over, test_stroke_t, type = "raw")

# Model Evaluation ::::::::

cm_nnet_over <- confusionMatrix(as.factor(y_hat_nnet_over), test_stroke_t$stroke)
cm_nnet_over
```

Its Accuracy is almost 77%. The Sensitivity exceeds 76%, and the Specificity is close to 78%. Balanced Accuracy is also around 77%

The area under de ROC curve is 0.81:

```{r sens_espec_ROC_nnet, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# Calc. Probs for every class
roc_nnet_over <- predict(train_nnet_over,
                         test_stroke_t, type = "prob")

# Sensitivity vs Specificity ROC CURVE
sens_espec_roc_nnet_over <- plot(roc(response = test_stroke_t$stroke, 
                                     predictor = roc_nnet_over$stroke, levels = c("no_stroke", "stroke")), 
                                 print.auc = TRUE,
                                 xlim = c(1,0),
                                 ylim = c(0,1),
                                 xlab = "Specificity", 
                                 ylab ="Sensibility", 
                                 main = "NNET - Over")

# Sensitivity vs Specificity AUC
auc(sens_espec_roc_nnet_over)
```

The selected parameters were Weight Decay = 0 and Hidden Units = 5:

```{r nnet_best_tunes, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}
plot(train_nnet_over)
```

#### 3.2.1.8 Flexible Discriminant Analysis

This model is another one that stands out, comparatively speaking. As we will see later, it is the one that gives the best results with the "better estimates" method to generate balanced training data.

With the oversampling method, the results are as follows:

```{r fda_over, echo=FALSE, message=FALSE, warning=FALSE}

# FDA #####
# Flexible Discriminant Analysis #####
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
train_fda_over <- train(stroke ~ ., method = "fda", 
                        data = train_stroke_over, 
                        trControl = ctrl,
                        metric="ROC")

y_hat_fda_over <- predict(train_fda_over, test_stroke_t)

# Model Evaluation ::::::::

cm_fda_over <- confusionMatrix(y_hat_fda_over, test_stroke_t$stroke)
cm_fda_over
```

Accuracy is close to 75%. Sensitivity is 74%, Specificity reaches 75%, and Balanced Accuracy is 74%

Area under the ROC curve reaches 0.82:

```{r sens_espec_ROC_fda, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# Calc. Probs for every class
roc_fda_over <- predict(train_fda_over,
                        test_stroke_t, type = "prob")

# Curve graph
sens_espec_roc_fda_over <- plot(roc(response = test_stroke_t$stroke, 
                                    predictor = roc_fda_over$stroke, levels = c("no_stroke", "stroke")), 
                                print.auc = TRUE,
                                xlim = c(1,0),
                                ylim = c(0,1),
                                xlab = "Specificity", 
                                ylab ="Sensibility", 
                                main = "Flexible Discriminant Analysis - Over")

# Sensitivity vs Specificity AUC
auc(sens_espec_roc_fda_over)
```

The final values used for the model were degree = 1 and nprune = 16.

#### 3.2.1.9 Naive Bayes

Finally, the Naives Bayes model is another that has some surprises in store. Its Accuracy is very high (greater than 93%), but the Sensitivity is very low (26%). Balanced Accuracy stays at 61%.

```{r nb_over, echo=FALSE, message=FALSE, warning=FALSE}

# NAIVE BAYES ######
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
train_naiveBayes_over <- train(stroke ~ ., method = "naive_bayes", 
                               data = train_stroke_over, 
                               trControl = ctrl,
                               metric="ROC")

y_hat_naiveBayes_over <- predict(train_naiveBayes_over, test_stroke_t)

# Model Evaluation ::::::::

cm_naiveBayes_over <- confusionMatrix(y_hat_naiveBayes_over, test_stroke_t$stroke)
cm_naiveBayes_over
```

Despite these results, the area under the ROC curve is close to 0.85. Once again, we note the next steps after the delivery of this report to analyze this result to fully understand it.

```{r sens_espec_ROC_nb, echo=FALSE, fig.align='center', fig.width=10, message=FALSE, warning=FALSE}

# Calc. Probs for every class
roc_naiveBayes_over <- predict(train_naiveBayes_over,
                               test_stroke_t, type = "prob")

# Sensitivity vs Specificity ROC CURVE
sens_espec_roc_naiveBayes_over <- plot(roc(response = test_stroke_t$stroke, 
                                           predictor = roc_naiveBayes_over$stroke, levels = c("no_stroke", "stroke")), 
                                       print.auc = TRUE,
                                       xlim = c(1,0),
                                       ylim = c(0,1),
                                       xlab = "Specificity", 
                                       ylab ="Sensibility", 
                                       main = "Naive Bayes - Over")

# Sensitivity vs Specificity AUC
```

The final values used for the model were laplace = 0, usekernel = TRUE and adjust = 1.

As we have already warned, the results, although better, are not excessively good. To find a sensitivity greater than 85%, together with a Specificity greater than 75%, we will have to wait for the FDA model with the better estimates method.

Below we summarize the results obtained with each model and the "both" and "better estimates" methods.

### 3.2.2 Both Method

#### 3.2.2.1 Gini Tree, CP = 0.01, minslit = 20, minbucket round 20/3, maxdepht = 30

#### 3.2.2.2 Gini Tree, CP = 0.001, minslit = 20, minbucket round 20/3, maxdepht = 3

#### 3.2.2.3 Default Gini Tree & Cost Matrix 3 to 1

#### 3.2.2.4 RPART caret

#### 3.2.2.5 K-Nearest-Neighbor

#### 3.2.2.6 Ramdom Forest

#### 3.2.2.7 Neural Network

#### 3.2.2.8 Flexible Discriminant Analysis

#### 3.2.2.9 Naive Bayes


### 3.2.3 Better Estimates Method

#### 3.2.3.1 Gini Tree, CP = 0.01, minslit = 20, minbucket round 20/3, maxdepht = 30

#### 3.2.3.2 Gini Tree, CP = 0.001, minslit = 20, minbucket round 20/3, maxdepht = 3

#### 3.2.3.3 Default Gini Tree & Cost Matrix 3 to 1

#### 3.2.3.4 RPART caret

#### 3.2.3.5 K-Nearest-Neighbor

#### 3.2.3.6 Ramdom Forest

#### 3.2.3.7 Neural Network

#### 3.2.3.8 Flexible Discriminant Analysis

#### 3.2.3.9 Naive Bayes


[^1]: https://www.kaggle.com/fedesoriano/stroke-prediction-dataset
[^2]: https://www.kaggle.com/fedesoriano
[^3]: Amat, Rodrigo: *Machine Learning con R y caret*. cienciadedatos.net, 2018: https://www.cienciadedatos.net/documentos/41_machine_learning_con_r_y_caret
[^4]: *Learn by Marketing*: https://www.learnbymarketing.com/tutorials/rpart-decision-trees-in-r/
[^5]: *Practical Guide to deal with Imbalanced Classification Problems in R*. Analytics Vidhya, 2016: https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/


